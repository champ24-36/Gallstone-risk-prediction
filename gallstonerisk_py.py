# -*- coding: utf-8 -*-
"""GallstoneRisk.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TC1IVUOobNb6BZYsYCW6bQcZ4dIu_NV4
"""

#==================model selection------all b
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
import lightgbm as lgb
import joblib

# ======================
# 1. Load Dataset
# ======================
df = pd.read_csv("/content/sample_data/gallstone_.csv")   # <--- change here

# Target column
target = "Gallstone Status"
X = df.drop(columns=[target])
y = df[target]

# ======================
# 2. Train-Test Split
# ======================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ======================
# 3. Logistic Regression
# ======================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg = LogisticRegression(max_iter=2000, class_weight="balanced")
logreg.fit(X_train_scaled, y_train)

# ======================
# 4. LightGBM
# ======================
lgbm = lgb.LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=-1,
    class_weight="balanced",
    random_state=42
)
lgbm.fit(X_train, y_train)

# ======================
# 5. Random Forest
# ======================
rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# ======================
# 6. XGBoost
# ======================
xgb_clf = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1,
    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),  # handle imbalance
    random_state=42,
    n_jobs=-1,
    use_label_encoder=False,
    eval_metric="logloss"
)
xgb_clf.fit(X_train, y_train)

# ======================
# 7. Support Vector Machine (SVM)
# ======================
svm = SVC(
    kernel="rbf",
    C=1.0,
    probability=True,
    class_weight="balanced",
    random_state=42
)
svm.fit(X_train_scaled, y_train)  # needs scaled data

# ======================
# 8. Decision Tree
# ======================
dt = DecisionTreeClassifier(
    max_depth=None,
    class_weight="balanced",
    random_state=42
)
dt.fit(X_train, y_train)

# ======================
# 9. K-Nearest Neighbors (KNN)
# ======================
knn = KNeighborsClassifier(
    n_neighbors=5,
    weights="distance",
    n_jobs=-1
)
knn.fit(X_train_scaled, y_train)

# ======================
# 10. Evaluation Function with Threshold
# ======================
def evaluate(model, X_test, y_test, threshold=0.5, is_scaled=False):
    if is_scaled:
        probs = model.predict_proba(X_test)[:, 1]
    else:
        probs = model.predict_proba(X_test)[:, 1]

    preds = (probs >= threshold).astype(int)

    return {
        "Accuracy": accuracy_score(y_test, preds),
        "Recall": recall_score(y_test, preds),
        "Precision": precision_score(y_test, preds),
        "F1 Score": f1_score(y_test, preds),
        "AUC Score": roc_auc_score(y_test, probs)
    }

# Default threshold = 0.5 (you can lower to 0.4 later)
threshold = 0.5

metrics_lr = evaluate(logreg, X_test_scaled, y_test, threshold=threshold, is_scaled=True)
metrics_lgb = evaluate(lgbm, X_test, y_test, threshold=threshold)
metrics_rf = evaluate(rf, X_test, y_test, threshold=threshold)
metrics_xgb = evaluate(xgb_clf, X_test, y_test, threshold=threshold)
metrics_svm = evaluate(svm, X_test_scaled, y_test, threshold=threshold, is_scaled=True)
metrics_dt = evaluate(dt, X_test, y_test, threshold=threshold)
metrics_knn = evaluate(knn, X_test_scaled, y_test, threshold=threshold, is_scaled=True)

# ======================
# 11. Results
# ======================
print(f"\n=== Logistic Regression (threshold={threshold}) ===")
for k, v in metrics_lr.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== LightGBM (threshold={threshold}) ===")
for k, v in metrics_lgb.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== Random Forest (threshold={threshold}) ===")
for k, v in metrics_rf.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== XGBoost (threshold={threshold}) ===")
for k, v in metrics_xgb.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== SVM (threshold={threshold}) ===")
for k, v in metrics_svm.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== Decision Tree (threshold={threshold}) ===")
for k, v in metrics_dt.items():
    print(f"{k}: {v:.4f}")

print(f"\n=== K-Nearest Neighbors (threshold={threshold}) ===")
for k, v in metrics_knn.items():
    print(f"{k}: {v:.4f}")

# ======================
# 12. Save Best Model by Recall
# ======================

# Collect recall scores into a dictionary
recall_scores = {
    "LogisticRegression": metrics_lr["Recall"],
    "LightGBM": metrics_lgb["Recall"],
    "RandomForest": metrics_rf["Recall"],
    "XGBoost": metrics_xgb["Recall"],
    "SVM": metrics_svm["Recall"],
    "DecisionTree": metrics_dt["Recall"],
    "KNN": metrics_knn["Recall"]
}

# Find the best model by recall
best_model_name = max(recall_scores, key=recall_scores.get)
best_recall = recall_scores[best_model_name]

print("\nüìä Recall Scores:", recall_scores)
print(f"üèÜ Best Model by Recall: {best_model_name} (Recall={best_recall:.4f})")

# Save the best model
if best_model_name == "LogisticRegression":
    joblib.dump(logreg, "gallstone_logreg.pkl")
    joblib.dump(scaler, "scaler.pkl")
    print("‚úÖ Best model (Logistic Regression) saved as gallstone_logreg.pkl and scaler.pkl")

elif best_model_name == "LightGBM":
    joblib.dump(lgbm, "gallstone_lgbm.pkl")
    print("‚úÖ Best model (LightGBM) saved as gallstone_lgbm.pkl")

elif best_model_name == "RandomForest":
    joblib.dump(rf, "gallstone_rf.pkl")
    print("‚úÖ Best model (Random Forest) saved as gallstone_rf.pkl")

elif best_model_name == "XGBoost":
    joblib.dump(xgb_clf, "gallstone_xgb.pkl")
    print("‚úÖ Best model (XGBoost) saved as gallstone_xgb.pkl")

elif best_model_name == "SVM":
    joblib.dump(svm, "gallstone_svm.pkl")
    joblib.dump(scaler, "scaler.pkl")
    print("‚úÖ Best model (SVM) saved as gallstone_svm.pkl and scaler.pkl")

elif best_model_name == "DecisionTree":
    joblib.dump(dt, "gallstone_dt.pkl")
    print("‚úÖ Best model (Decision Tree) saved as gallstone_dt.pkl")

elif best_model_name == "KNN":
    joblib.dump(knn, "gallstone_knn.pkl")
    joblib.dump(scaler, "scaler.pkl")
    print("‚úÖ Best model (KNN) saved as gallstone_knn.pkl and scaler.pkl")

#===========model training + report---yayy finally
import os
import pickle
import joblib
import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from xgboost import callback
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import warnings

warnings.filterwarnings('ignore')

class LightGBMContinuousTrainer:
    def __init__(self, data_path=None, model_path="bgallstone_lgbm.pkl", prep_path="gallstone_xgb_prep.pkl"):
        self.data_path = data_path
        self.model_path = model_path
        self.prep_path = prep_path
        self.model = None
        self.label_encoders = {}
        self.imputers = {}
        self.feature_names = None
        self.target_col = "Gallstone Status"  # Fixed target column name
        self.numeric_cols = []
        self.categorical_cols = []

    def load_data(self):
        """Load CSV data"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Data file not found: {self.data_path}")

        print(f"üìÇ Loading data from {self.data_path}")
        df = pd.read_csv(self.data_path)
        print(f"üìä Data loaded successfully. Shape: {df.shape}")
        print(f"üìä Columns: {list(df.columns)}")

        # Check if target column exists
        if self.target_col not in df.columns:
            print(f"‚ö†Ô∏è Target column '{self.target_col}' not found. Available columns:")
            for i, col in enumerate(df.columns):
                print(f"  {i}: {col}")
            # Try common variations
            possible_targets = [col for col in df.columns if 'gallstone' in col.lower()]
            if possible_targets:
                self.target_col = possible_targets[0]
                print(f"‚úÖ Using '{self.target_col}' as target column")
            else:
                raise ValueError(f"Could not find target column. Please specify the correct target column name.")

        return df

    def prepare_data(self, df, is_training=True):
        """Prepare features and target for training/prediction"""
        # Create feature matrix
        X = df.drop(columns=[self.target_col], errors="ignore").copy()
        y = df[self.target_col].copy() if self.target_col in df.columns else None

        # Replace various representations of missing values with np.nan
        missing_values = ["None", "none", "NONE", "", "NaN", "nan", "null", "NULL", "missing"]
        for col in X.columns:
            X[col] = X[col].replace(missing_values, np.nan)

        if is_training:
            # Identify column types during training
            self.numeric_cols = []
            self.categorical_cols = []

            for col in X.columns:
                # Try to convert to numeric
                sample_vals = X[col].dropna()
                if len(sample_vals) == 0:
                    # All missing values, treat as categorical
                    self.categorical_cols.append(col)
                    continue

                try:
                    # Try converting a sample to float
                    pd.to_numeric(sample_vals.iloc[:min(10, len(sample_vals))])
                    # If successful and original dtype suggests numeric, treat as numeric
                    if X[col].dtype in ['int64', 'float64'] or pd.api.types.is_numeric_dtype(X[col]):
                        self.numeric_cols.append(col)
                    else:
                        # Check if most values can be converted to numeric
                        try:
                            converted = pd.to_numeric(sample_vals, errors='coerce')
                            non_null_ratio = converted.notna().sum() / len(converted)
                            if non_null_ratio > 0.8:  # 80% of values can be converted
                                self.numeric_cols.append(col)
                            else:
                                self.categorical_cols.append(col)
                        except:
                            self.categorical_cols.append(col)
                except:
                    self.categorical_cols.append(col)

            print(f"üìä Identified {len(self.numeric_cols)} numeric and {len(self.categorical_cols)} categorical columns")
            print(f"üìä Numeric columns: {self.numeric_cols}")
            print(f"üìä Categorical columns: {self.categorical_cols}")

        # Process numeric features
        for col in self.numeric_cols:
            # Convert to numeric, coercing errors to NaN
            X[col] = pd.to_numeric(X[col], errors="coerce")

            imputer_key = f"{col}_numeric"
            if is_training:
                self.imputers[imputer_key] = SimpleImputer(strategy="median")
                X[col] = self.imputers[imputer_key].fit_transform(X[[col]]).flatten()
            else:
                if imputer_key in self.imputers:
                    X[col] = self.imputers[imputer_key].transform(X[[col]]).flatten()
                else:
                    # Fallback: fill with median
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        median_val = 0
                    X[col].fillna(median_val, inplace=True)

        # Process categorical features
        for col in self.categorical_cols:
            # Convert to string
            X[col] = X[col].astype(str)
            # Replace 'nan' strings back to proper NaN for imputation
            X[col] = X[col].replace('nan', np.nan)

            imputer_key = f"{col}_categorical"
            if is_training:
                self.imputers[imputer_key] = SimpleImputer(strategy="most_frequent")
                X[col] = self.imputers[imputer_key].fit_transform(X[[col]]).flatten()

                # Fit label encoder
                self.label_encoders[col] = LabelEncoder()
                X[col] = self.label_encoders[col].fit_transform(X[col])
            else:
                # Apply imputation
                if imputer_key in self.imputers:
                    X[col] = self.imputers[imputer_key].transform(X[[col]]).flatten()
                else:
                    # Fallback: fill with most frequent
                    mode_vals = X[col].mode()
                    mode_val = mode_vals[0] if len(mode_vals) > 0 else "Unknown"
                    X[col].fillna(mode_val, inplace=True)

                # Apply label encoding
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    def safe_transform(x):
                        if x in le.classes_:
                            return le.transform([x])[0]
                        else:
                            # Return the encoding for the most frequent class
                            return le.transform([le.classes_[0]])[0]

                    X[col] = X[col].apply(safe_transform)
                else:
                    # Fallback: create new encoder (not ideal but prevents crashes)
                    temp_encoder = LabelEncoder()
                    X[col] = temp_encoder.fit_transform(X[col])

        # Store feature names during training
        if is_training:
            self.feature_names = X.columns.tolist()

        # Ensure column order matches training
        if self.feature_names and not is_training:
            missing_cols = [col for col in self.feature_names if col not in X.columns]
            if missing_cols:
                print(f"‚ö†Ô∏è Missing columns in prediction data: {missing_cols}")
                # Add missing columns with default values
                for col in missing_cols:
                    if col in self.numeric_cols:
                        X[col] = 0
                    else:
                        X[col] = 0  # Encoded categorical default

            # Reorder columns to match training
            X = X[self.feature_names]

        # Handle target variable
        if y is not None:
            if y.dtype == 'object' or pd.api.types.is_string_dtype(y):
                if is_training:
                    self.label_encoders['target'] = LabelEncoder()
                    y = self.label_encoders['target'].fit_transform(y.astype(str))
                else:
                    if 'target' in self.label_encoders:
                        le = self.label_encoders['target']
                        y = y.astype(str).apply(lambda x: x if x in le.classes_ else le.classes_[0])
                        y = le.transform(y)
                    else:
                        y = pd.Categorical(y).codes
            else:
                y = y.astype(int)

        return X, y

    def save(self):
        """Save model and preprocessing objects"""
        try:
            # Create directories if they don't exist
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True) if os.path.dirname(self.model_path) else None
            os.makedirs(os.path.dirname(self.prep_path), exist_ok=True) if os.path.dirname(self.prep_path) else None

            # Save model
            joblib.dump(self.model, self.model_path)

            # Save preprocessing objects
            joblib.dump({
                "label_encoders": self.label_encoders,
                "imputers": self.imputers,
                "feature_names": self.feature_names,
                "numeric_cols": self.numeric_cols,
                "categorical_cols": self.categorical_cols,
                "target_col": self.target_col
            }, self.prep_path)

            print(f"üíæ Model saved to {self.model_path}")
            print(f"üíæ Preprocessing saved to {self.prep_path}")
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")

    def load(self):
        """Load existing model and preprocessing objects"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.prep_path):
                # Load model
                self.model = joblib.load(self.model_path)

                # Load preprocessing objects
                prep = joblib.load(self.prep_path)
                self.label_encoders = prep.get("label_encoders", {})
                self.imputers = prep.get("imputers", {})
                self.feature_names = prep.get("feature_names", None)
                self.numeric_cols = prep.get("numeric_cols", [])
                self.categorical_cols = prep.get("categorical_cols", [])
                self.target_col = prep.get("target_col", self.target_col)

                print(f"‚úÖ Loaded existing model and preprocessing from {self.model_path}")
                return True
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading model: {e}")
            return False

    def train(self, df, test_size=0.2, random_state=42):
        """Train the model"""
        print("üöÄ Starting training process...")

        # Try to load existing model
        model_exists = self.load()

        # Prepare data
        X, y = self.prepare_data(df, is_training=True)

        # Check class distribution
        unique_classes, counts = np.unique(y, return_counts=True)
        class_dist = dict(zip(unique_classes, counts))
        print(f"üìä Class distribution: {class_dist}")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        print(f"üìà Training set size: {len(y_train)}")
        print(f"üìà Test set size: {len(y_test)}")

        # Initialize or use existing model
        if not model_exists:
            print("üöÄ Training new model...")
            self.model = XGBClassifier(
                n_estimators=1000,
                learning_rate=0.05,
                random_state=random_state,
                objective='binary:logistic',
                eval_metric='logloss',   # required for eval_set
                use_label_encoder=False
            )
        else:
            print("üöÄ Continuing training with existing model...")

        # Train model
        try:
            if model_exists:
                # Continue training from existing model
                self.model = XGBClassifier(
                n_estimators=1000,
                learning_rate=0.05,
                random_state=random_state,
                objective='binary:logistic',
                eval_metric=["logloss", "auc", "error"],  # multiple metrics
                use_label_encoder=False
            )

            self.model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=False
            )
            print("‚úÖ Training complete")
        except Exception as e:
            print(f"‚ùå Training error: {e}")
            # Fallback to basic training
            self.model.fit(X_train, y_train)

        # Evaluate model
        try:
            preds = self.model.predict(X_test)
            pred_proba = self.model.predict_proba(X_test)[:, 1]

            acc = accuracy_score(y_test, preds)
            print("\nüìä Model Performance:")
            print(f"Accuracy: {acc:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_test, preds))
            print("Confusion Matrix:")
            print(confusion_matrix(y_test, preds))
        except Exception as e:
            print(f"‚ö†Ô∏è Evaluation error: {e}")

        # Save artifacts
        self.save()

        # Feature importance
        try:
            importances = pd.DataFrame({
                "feature": self.feature_names,
                "importance": self.model.feature_importances_
            }).sort_values(by="importance", ascending=False)
            print("\nüéØ Top 10 Most Important Features:")
            print(importances.head(10))
        except Exception as e:
            print(f"‚ö†Ô∏è Feature importance error: {e}")

    def predict_single(self, patient_dict):
        """Predict for a single patient"""
        if self.model is None:
            raise ValueError("Model not loaded. Please train or load a model first.")

        try:
            # Convert to DataFrame
            df = pd.DataFrame([patient_dict])

            # Prepare data
            X, _ = self.prepare_data(df, is_training=False)

            # Make prediction
            pred_proba = self.model.predict_proba(X)
            pred = self.model.predict(X)

            # Return results
            probability = pred_proba[0][1] if pred_proba.shape[1] > 1 else pred_proba[0][0]
            prediction = int(pred[0])

            return prediction, probability

        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            return 0, 0.0

    def predict_batch(self, df):
        """Predict for multiple patients"""
        if self.model is None:
            raise ValueError("Model not loaded. Please train or load a model first.")

        try:
            X, _ = self.prepare_data(df, is_training=False)
            pred_proba = self.model.predict_proba(X)
            preds = self.model.predict(X)

            probabilities = pred_proba[:, 1] if pred_proba.shape[1] > 1 else pred_proba[:, 0]

            return preds, probabilities
        except Exception as e:
            print(f"‚ùå Batch prediction error: {e}")
            return np.array([]), np.array([])


def main():
    """Main execution function"""
    # File paths - update these to match your environment
    data_path = "/content/sample_data/gallstone_.csv"
    model_path = "/content/gallstone_xgb.pkl"
    prep_path = "/content/gallstone_xgb_prep.pkl"

    try:
        # Initialize trainer
        trainer = LightGBMContinuousTrainer(data_path, model_path, prep_path)

        # Load and train
        df = trainer.load_data()
        trainer.train(df)

        # Example prediction with proper missing value handling
        patient_example = {
            'Age': 45, 'Gender': 'Male', 'Comorbidity': None,  # Use None instead of np.nan
            'Coronary Artery Disease (CAD)': 'No', 'Hypothyroidism': 'No',
            'Hyperlipidemia': 'No', 'Diabetes Mellitus (DM)': 'No',
            'Height': 170, 'Weight': 70, 'Body Mass Index (BMI)': 24.2,
            'Total Body Water (TBW)': 40, 'Extracellular Water (ECW)': 15,
            'Intracellular Water (ICW)': 25,
            'Extracellular Fluid/Total Body Water (ECF/TBW)': 0.375,
            'Total Body Fat Ratio (TBFR) (%)': 20, 'Lean Mass (LM) (%)': 70,
            'Body Protein Content (Protein) (%)': 15, 'Visceral Fat Rating (VFR)': 10,
            'Bone Mass (BM)': 3, 'Muscle Mass (MM)': 35, 'Obesity (%)': 15,
            'Total Fat Content (TFC)': 14, 'Visceral Fat Area (VFA)': 80,
            'Visceral Muscle Area (VMA) (Kg)': 30, 'Hepatic Fat Accumulation (HFA)': 5,
            'Glucose': 90, 'Total Cholesterol (TC)': 180,
            'Low Density Lipoprotein (LDL)': 100, 'High Density Lipoprotein (HDL)': 50,
            'Triglyceride': 120, 'Aspartat Aminotransferaz (AST)': 25,
            'Alanin Aminotransferaz (ALT)': 22, 'Alkaline Phosphatase (ALP)': 80,
            'Creatinine': 1.0, 'Glomerular Filtration Rate (GFR)': 90,
            'C-Reactive Protein (CRP)': 0.3, 'Hemoglobin (HGB)': 14, 'Vitamin D': 30
        }

        print("\nüîÆ Making prediction for example patient...")
        pred, proba = trainer.predict_single(patient_example)

        # Interpret results
        risk_level = "High" if proba > 0.7 else "Medium" if proba > 0.3 else "Low"
        result = "Positive (Has Gallstones)" if pred == 1 else "Negative (No Gallstones)"

        print(f"Prediction: {result}")
        print(f"Probability: {proba:.4f}")
        print(f"Risk Level: {risk_level}")

    except Exception as e:
        print(f"‚ùå An error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

#=============KAIV NO ERRORS NEVER LOSE THIS [incsert crying emoji]:)===above code + XAI (plots and shap)
import os
import pickle
import joblib
import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import shap

warnings.filterwarnings('ignore')
plt.style.use('default')

class LightGBMContinuousTrainer:
    def __init__(self, data_path=None, model_path="bgallstone_lgbm.pkl", prep_path="gallstone_xgb_prep.pkl"):
        self.data_path = data_path
        self.model_path = model_path
        self.prep_path = prep_path
        self.model = None
        self.label_encoders = {}
        self.imputers = {}
        self.feature_names = None
        self.target_col = "Gallstone Status"  # Fixed target column name
        self.numeric_cols = []
        self.categorical_cols = []
        self.explainer = None
        self.X_test_sample = None  # Store test sample for SHAP background

    def load_data(self):
        """Load CSV data"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Data file not found: {self.data_path}")

        print(f"üìÇ Loading data from {self.data_path}")
        df = pd.read_csv(self.data_path)
        print(f"üìä Data loaded successfully. Shape: {df.shape}")
        print(f"üìä Columns: {list(df.columns)}")

        # Check if target column exists
        if self.target_col not in df.columns:
            print(f"‚ö†Ô∏è Target column '{self.target_col}' not found. Available columns:")
            for i, col in enumerate(df.columns):
                print(f"  {i}: {col}")
            # Try common variations
            possible_targets = [col for col in df.columns if 'gallstone' in col.lower()]
            if possible_targets:
                self.target_col = possible_targets[0]
                print(f"‚úÖ Using '{self.target_col}' as target column")
            else:
                raise ValueError(f"Could not find target column. Please specify the correct target column name.")

        return df

    def prepare_data(self, df, is_training=True):
        """Prepare features and target for training/prediction"""
        # Create feature matrix
        X = df.drop(columns=[self.target_col], errors="ignore").copy()
        y = df[self.target_col].copy() if self.target_col in df.columns else None

        # Replace various representations of missing values with np.nan
        missing_values = ["None", "none", "NONE", "", "NaN", "nan", "null", "NULL", "missing"]
        for col in X.columns:
            X[col] = X[col].replace(missing_values, np.nan)

        if is_training:
            # Identify column types during training
            self.numeric_cols = []
            self.categorical_cols = []

            for col in X.columns:
                # Try to convert to numeric
                sample_vals = X[col].dropna()
                if len(sample_vals) == 0:
                    # All missing values, treat as categorical
                    self.categorical_cols.append(col)
                    continue

                try:
                    # Try converting a sample to float
                    pd.to_numeric(sample_vals.iloc[:min(10, len(sample_vals))])
                    # If successful and original dtype suggests numeric, treat as numeric
                    if X[col].dtype in ['int64', 'float64'] or pd.api.types.is_numeric_dtype(X[col]):
                        self.numeric_cols.append(col)
                    else:
                        # Check if most values can be converted to numeric
                        try:
                            converted = pd.to_numeric(sample_vals, errors='coerce')
                            non_null_ratio = converted.notna().sum() / len(converted)
                            if non_null_ratio > 0.8:  # 80% of values can be converted
                                self.numeric_cols.append(col)
                            else:
                                self.categorical_cols.append(col)
                        except:
                            self.categorical_cols.append(col)
                except:
                    self.categorical_cols.append(col)

            print(f"üìä Identified {len(self.numeric_cols)} numeric and {len(self.categorical_cols)} categorical columns")
            print(f"üìä Numeric columns: {self.numeric_cols}")
            print(f"üìä Categorical columns: {self.categorical_cols}")

        # Process numeric features
        for col in self.numeric_cols:
            # Convert to numeric, coercing errors to NaN
            X[col] = pd.to_numeric(X[col], errors="coerce")

            imputer_key = f"{col}_numeric"
            if is_training:
                self.imputers[imputer_key] = SimpleImputer(strategy="median")
                X[col] = self.imputers[imputer_key].fit_transform(X[[col]]).flatten()
            else:
                if imputer_key in self.imputers:
                    X[col] = self.imputers[imputer_key].transform(X[[col]]).flatten()
                else:
                    # Fallback: fill with median
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        median_val = 0
                    X[col].fillna(median_val, inplace=True)

        # Process categorical features
        for col in self.categorical_cols:
            # Convert to string
            X[col] = X[col].astype(str)
            # Replace 'nan' strings back to proper NaN for imputation
            X[col] = X[col].replace('nan', np.nan)

            imputer_key = f"{col}_categorical"
            if is_training:
                self.imputers[imputer_key] = SimpleImputer(strategy="most_frequent")
                X[col] = self.imputers[imputer_key].fit_transform(X[[col]]).flatten()

                # Fit label encoder
                self.label_encoders[col] = LabelEncoder()
                X[col] = self.label_encoders[col].fit_transform(X[col])
            else:
                # Apply imputation
                if imputer_key in self.imputers:
                    X[col] = self.imputers[imputer_key].transform(X[[col]]).flatten()
                else:
                    # Fallback: fill with most frequent
                    mode_vals = X[col].mode()
                    mode_val = mode_vals[0] if len(mode_vals) > 0 else "Unknown"
                    X[col].fillna(mode_val, inplace=True)

                # Apply label encoding
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    def safe_transform(x):
                        if x in le.classes_:
                            return le.transform([x])[0]
                        else:
                            # Return the encoding for the most frequent class
                            return le.transform([le.classes_[0]])[0]

                    X[col] = X[col].apply(safe_transform)
                else:
                    # Fallback: create new encoder (not ideal but prevents crashes)
                    temp_encoder = LabelEncoder()
                    X[col] = temp_encoder.fit_transform(X[col])

        # Store feature names during training
        if is_training:
            self.feature_names = X.columns.tolist()

        # Ensure column order matches training
        if self.feature_names and not is_training:
            missing_cols = [col for col in self.feature_names if col not in X.columns]
            if missing_cols:
                print(f"‚ö†Ô∏è Missing columns in prediction data: {missing_cols}")
                # Add missing columns with default values
                for col in missing_cols:
                    if col in self.numeric_cols:
                        X[col] = 0
                    else:
                        X[col] = 0  # Encoded categorical default

            # Reorder columns to match training
            X = X[self.feature_names]

        # Handle target variable
        if y is not None:
            if y.dtype == 'object' or pd.api.types.is_string_dtype(y):
                if is_training:
                    self.label_encoders['target'] = LabelEncoder()
                    y = self.label_encoders['target'].fit_transform(y.astype(str))
                else:
                    if 'target' in self.label_encoders:
                        le = self.label_encoders['target']
                        y = y.astype(str).apply(lambda x: x if x in le.classes_ else le.classes_[0])
                        y = le.transform(y)
                    else:
                        y = pd.Categorical(y).codes
            else:
                y = y.astype(int)

        return X, y

    def plot_model_performance(self, y_test, y_pred, y_pred_proba):
        """Create comprehensive performance visualizations"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
        axes[0,0].set_title('Confusion Matrix')
        axes[0,0].set_xlabel('Predicted')
        axes[0,0].set_ylabel('Actual')

        # 2. ROC Curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc_score = roc_auc_score(y_test, y_pred_proba)
        axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')
        axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[0,1].set_xlim([0.0, 1.0])
        axes[0,1].set_ylim([0.0, 1.05])
        axes[0,1].set_xlabel('False Positive Rate')
        axes[0,1].set_ylabel('True Positive Rate')
        axes[0,1].set_title('ROC Curve')
        axes[0,1].legend(loc="lower right")

        # 3. Probability Distribution
        axes[1,0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Gallstones', color='blue')
        axes[1,0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Gallstones', color='red')
        axes[1,0].set_xlabel('Predicted Probability')
        axes[1,0].set_ylabel('Frequency')
        axes[1,0].set_title('Probability Distribution by Class')
        axes[1,0].legend()

        # 4. Feature Importance
        if hasattr(self.model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=True).tail(15)

            axes[1,1].barh(range(len(importance_df)), importance_df['importance'])
            axes[1,1].set_yticks(range(len(importance_df)))
            axes[1,1].set_yticklabels(importance_df['feature'])
            axes[1,1].set_xlabel('Feature Importance')
            axes[1,1].set_title('Top 15 Most Important Features')

        plt.tight_layout()
        plt.show()

        # Print detailed metrics
        print("\n" + "="*50)
        print("üéØ DETAILED MODEL PERFORMANCE METRICS")
        print("="*50)
        print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(f"AUC Score: {auc_score:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))

    def initialize_explainer(self, X_sample):
        """Initialize SHAP explainer with proper error handling"""
        try:
            print("üîç Initializing SHAP explainer...")

            # Method 1: Try TreeExplainer with background data (without check_additivity parameter)
            try:
                background_size = min(100, len(X_sample))
                background = X_sample.sample(n=background_size, random_state=42)
                self.explainer = shap.TreeExplainer(self.model, background)
                print("‚úÖ SHAP explainer initialized with background data")
                return
            except Exception as e1:
                print(f"‚ö†Ô∏è TreeExplainer with background failed: {e1}")

            # Method 2: Try TreeExplainer without background data
            try:
                self.explainer = shap.TreeExplainer(self.model)
                print("‚úÖ SHAP explainer initialized without background data")
                return
            except Exception as e2:
                print(f"‚ö†Ô∏è TreeExplainer without background failed: {e2}")

            # Method 3: Try newer SHAP API with Explainer
            try:
                background_size = min(50, len(X_sample))
                background = X_sample.sample(n=background_size, random_state=42)
                self.explainer = shap.Explainer(self.model, background)
                print("‚úÖ SHAP general Explainer initialized")
                return
            except Exception as e3:
                print(f"‚ö†Ô∏è General Explainer failed: {e3}")

            # Method 4: KernelExplainer as last resort
            try:
                background_size = min(20, len(X_sample))
                background = X_sample.sample(n=background_size, random_state=42)
                self.explainer = shap.KernelExplainer(
                    lambda x: self.model.predict_proba(x)[:, 1],
                    background
                )
                print("‚úÖ SHAP KernelExplainer initialized (fallback)")
                return
            except Exception as e4:
                print(f"‚ö†Ô∏è KernelExplainer failed: {e4}")

            print("‚ùå All SHAP initialization methods failed")
            self.explainer = None

        except Exception as e:
            print(f"‚ùå Fatal error initializing SHAP explainer: {e}")
            self.explainer = None

    def plot_shap_summary(self, X_test_sample=None, max_display=20):
        """Create SHAP summary plots with robust error handling"""
        if self.explainer is None:
            print("‚ùå SHAP explainer not initialized")
            self._plot_feature_importance_fallback(max_display)
            return

        try:
            # Use stored test sample or provided sample
            if X_test_sample is None and self.X_test_sample is not None:
                X_test_sample = self.X_test_sample
            elif X_test_sample is None:
                print("‚ùå No test sample available for SHAP analysis")
                self._plot_feature_importance_fallback(max_display)
                return

            print("üìä Computing SHAP values...")
            # Limit sample size for performance
            sample_size = min(100, len(X_test_sample))
            X_shap_sample = X_test_sample.sample(n=sample_size, random_state=42)

        except Exception as e:
            print(f"‚ùå Error creating SHAP summary : {e}")
            self._plot_feature_importance_fallback(max_display)

    def _compute_shap_values_safely(self, X_sample, explainer_type="summary",max_display=50):
        """
        Safely compute SHAP values with multiple fallback strategies
        Args:
            X_sample: Input data
            explainer_type: "summary" or "individual" for different error handling
        Returns:
            shap_values or None if all methods fail
        """
        if self.explainer is None:
            return None

        methods_to_try = []

        if isinstance(self.explainer, shap.TreeExplainer):
            methods_to_try = [
                # Method 1: TreeExplainer.shap_values with check_additivity=False
                lambda x: self.explainer.shap_values(x, check_additivity=False),
                # Method 2: TreeExplainer.shap_values without check_additivity
                lambda x: self.explainer.shap_values(x),
                # Method 3: TreeExplainer callable with check_additivity=False
                lambda x: self.explainer(x, check_additivity=False).values,
                # Method 4: TreeExplainer callable without check_additivity
                lambda x: self.explainer(x).values,
            ]
        elif isinstance(self.explainer, shap.KernelExplainer):
            methods_to_try = [
                lambda x: self.explainer.shap_values(x),
            ]
        else:
            # General explainer
            methods_to_try = [
                lambda x: self.explainer(x).values,
                lambda x: self.explainer.shap_values(x) if hasattr(self.explainer, 'shap_values') else None,
            ]

        for i, method in enumerate(methods_to_try):
            try:
                if method is None:
                    continue
                shap_values = method(X_sample)
                if shap_values is not None:
                    print(f"‚úÖ SHAP values computed using method {i+1}")
                    return shap_values
            except TypeError as e:
                if "check_additivity" in str(e):
                    continue  # Try next method
                else:
                    print(f"‚ö†Ô∏è Method {i+1} failed with TypeError: {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è Method {i+1} failed: {e}")
                continue

                print("‚ùå All SHAP computation methods failed")
                return None

            if shap_values is None:
                    print("‚ùå Failed to compute SHAP values")
                    self._plot_feature_importance_fallback(max_display)
                    return

            # Handle binary classification output format
            if isinstance(shap_values, list) and len(shap_values) == 2:
                    shap_values = shap_values[1]  # Use positive class
            elif len(shap_values.shape) == 3:
                    shap_values = shap_values[:, :, 1]  # Use positive class for binary
            try:
            # Create plots
                fig, axes = plt.subplots(2, 1, figsize=(12, 16))

            # Summary plot (bar)
                plt.sca(axes[0])
                shap.summary_plot(shap_values, X_shap_sample,
                            plot_type="bar", max_display=max_display, show=False)
                axes[0].set_title('SHAP Feature Importance (Global)', fontsize=14)

            # Summary plot (beeswarm)
                plt.sca(axes[1])
                shap.summary_plot(shap_values, X_shap_sample,
                            max_display=max_display, show=False)
                axes[1].set_title('SHAP Summary Plot (Feature Impact)', fontsize=14)

                plt.tight_layout()
                plt.show()

                print("‚úÖ SHAP summary plots created successfully")

            except Exception as e:
                print(f"‚ùå Error creating SHAP summary plots: {e}")
                self._compute_shap_values_safely(max_display)

    def _plot_feature_importance_fallback(self, max_display=20):
        """Fallback feature importance plot when SHAP fails"""
        print("üìä Creating fallback feature importance plot...")
        try:
            if hasattr(self.model, 'feature_importances_'):
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': self.model.feature_importances_
                }).sort_values('importance', ascending=True).tail(max_display)

                plt.figure(figsize=(10, 8))
                plt.barh(range(len(importance_df)), importance_df['importance'])
                plt.yticks(range(len(importance_df)), importance_df['feature'])
                plt.xlabel('Feature Importance')
                plt.title('Model Feature Importance (LightGBM native)')
                plt.tight_layout()
                plt.show()
                print("‚úÖ Fallback feature importance plot created")
            else:
                print("‚ùå Model does not have feature_importances_ attribute")
        except Exception as e:
            print(f"‚ùå Fallback plot also failed: {e}")

    def explain_patient_prediction(self, patient_dict, show_plots=True):
        """Provide detailed explanation for a single patient with robust error handling"""
        try:
            # Convert to DataFrame and prepare
            df = pd.DataFrame([patient_dict])
            X, _ = self.prepare_data(df, is_training=False)

            # Make prediction
            pred_proba = self.model.predict_proba(X)[0]
            pred = self.model.predict(X)[0]
            probability = pred_proba[1] if len(pred_proba) > 1 else pred_proba[0]

            print("\n" + "="*60)
            print("üîç INDIVIDUAL PATIENT PREDICTION")
            print("="*60)
            print(f"Prediction: {'Positive (Has Gallstones)' if pred == 1 else 'Negative (No Gallstones)'}")
            print(f"Probability: {probability:.4f}")
            print(f"Risk Level: {'High' if probability > 0.7 else 'Medium' if probability > 0.3 else 'Low'}")

            # Try to compute SHAP values if explainer exists
            shap_values_single = None
            if self.explainer is not None:
                try:
                    print("\nüìä Computing SHAP explanation...")

                    # Use the safe SHAP computation method
                    shap_values = self._compute_shap_values_safely(X, "individual")

                    if shap_values is not None:
                        # Extract single patient values
                        if isinstance(shap_values, list) and len(shap_values) == 2:
                            shap_values_single = shap_values[1][0]  # Positive class, first sample
                        elif len(shap_values.shape) == 3:
                            shap_values_single = shap_values[0, :, 1]  # First sample, positive class
                        else:
                            shap_values_single = shap_values[0]

                        if show_plots and shap_values_single is not None:
                            self._create_explanation_plots(X, shap_values_single, probability, pred)
                    else:
                        print("‚ö†Ô∏è SHAP computation failed, using fallback explanation")
                        shap_values_single = None

                except Exception as e:
                    print(f"‚ö†Ô∏è Could not compute SHAP values: {e}")

            # Fallback explanation using feature importance
            if shap_values_single is None and show_plots:
                self._create_importance_based_explanation(X, probability, pred)

            return pred, probability, shap_values_single

        except Exception as e:
            print(f"‚ùå Error in patient explanation: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None

    def _create_explanation_plots(self, X, shap_values_single, probability, pred):
        """Create SHAP-based explanation plots"""
        try:
            fig, axes = plt.subplots(2, 1, figsize=(12, 10))

            # Feature contribution bar plot
            plt.sca(axes[0])
            shap_df = pd.DataFrame({
                'feature': self.feature_names,
                'shap_value': shap_values_single,
                'feature_value': X.iloc[0].values
            })
            shap_df['abs_shap'] = np.abs(shap_df['shap_value'])
            top_features = shap_df.nlargest(15, 'abs_shap')

            colors = ['red' if x > 0 else 'blue' for x in top_features['shap_value']]
            axes[0].barh(range(len(top_features)), top_features['shap_value'], color=colors)
            axes[0].set_yticks(range(len(top_features)))
            axes[0].set_yticklabels([f"{feat}\n(val: {val:.2f})"
                                    for feat, val in zip(top_features['feature'],
                                                        top_features['feature_value'])])
            axes[0].set_xlabel('SHAP Value (Impact on Prediction)')
            axes[0].set_title('Top Contributing Features for This Patient')
            axes[0].axvline(x=0, color='black', linestyle='-', alpha=0.3)

            # Cumulative impact plot
            plt.sca(axes[1])
            cumulative_impact = np.cumsum(top_features['shap_value'].values)
            axes[1].plot(range(len(top_features)), cumulative_impact, 'b-', marker='o')
            axes[1].set_xticks(range(len(top_features)))
            axes[1].set_xticklabels(top_features['feature'].values, rotation=45, ha='right')
            axes[1].set_ylabel('Cumulative SHAP Impact')
            axes[1].set_title('Cumulative Feature Impact on Prediction')
            axes[1].grid(True, alpha=0.3)
            axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)

            plt.tight_layout()
            plt.show()

            print("\nüéØ TOP CONTRIBUTING FACTORS:")
            for idx, row in top_features.head(10).iterrows():
                direction = "increases" if row['shap_value'] > 0 else "decreases"
                print(f"  ‚Ä¢ {row['feature']} (value: {row['feature_value']:.2f}): "
                      f"{direction} risk by {abs(row['shap_value']):.4f}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error creating SHAP explanation plots: {e}")

    def _create_importance_based_explanation(self, X, probability, pred):
        """Create fallback explanation using feature importance"""
        try:
            if hasattr(self.model, 'feature_importances_'):
                print("\nüìä Creating importance-based explanation (SHAP unavailable)...")

                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': self.model.feature_importances_,
                    'value': X.iloc[0].values
                }).sort_values('importance', ascending=False).head(15)

                plt.figure(figsize=(10, 6))
                plt.barh(range(len(importance_df)), importance_df['importance'])
                plt.yticks(range(len(importance_df)),
                          [f"{feat}\n(val: {val:.2f})"
                           for feat, val in zip(importance_df['feature'],
                                              importance_df['value'])])
                plt.xlabel('Feature Importance')
                plt.title(f'Important Features for Prediction\n'
                         f'Prediction: {"Gallstones" if pred == 1 else "No Gallstones"} '
                         f'(Probability: {probability:.2f})')
                plt.tight_layout()
                plt.show()

                print("\nüéØ MOST IMPORTANT FEATURES (by model importance):")
                for idx, row in importance_df.head(10).iterrows():
                    print(f"  ‚Ä¢ {row['feature']}: value={row['value']:.2f}, "
                          f"importance={row['importance']:.4f}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error creating importance-based explanation: {e}")

    def save(self):
        """Save model and preprocessing objects"""
        try:
            # Create directories if they don't exist
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True) if os.path.dirname(self.model_path) else None
            os.makedirs(os.path.dirname(self.prep_path), exist_ok=True) if os.path.dirname(self.prep_path) else None

            # Save model
            joblib.dump(self.model, self.model_path)

            # Save preprocessing objects
            joblib.dump({
                "label_encoders": self.label_encoders,
                "imputers": self.imputers,
                "feature_names": self.feature_names,
                "numeric_cols": self.numeric_cols,
                "categorical_cols": self.categorical_cols,
                "target_col": self.target_col
            }, self.prep_path)

            print(f"üíæ Model saved to {self.model_path}")
            print(f"üíæ Preprocessing saved to {self.prep_path}")
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")

    def load(self):
        """Load existing model and preprocessing objects"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.prep_path):
                # Load model
                self.model = joblib.load(self.model_path)

                # Load preprocessing objects
                prep = joblib.load(self.prep_path)
                self.label_encoders = prep.get("label_encoders", {})
                self.imputers = prep.get("imputers", {})
                self.feature_names = prep.get("feature_names", None)
                self.numeric_cols = prep.get("numeric_cols", [])
                self.categorical_cols = prep.get("categorical_cols", [])
                self.target_col = prep.get("target_col", self.target_col)

                print(f"‚úÖ Loaded existing model and preprocessing from {self.model_path}")
                return True
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading model: {e}")
            return False

    def train(self, df, test_size=0.2, random_state=42):
        """Train the model with enhanced visualization and explanations"""
        print("üöÄ Starting training process...")

        # Try to load existing model
        model_exists = self.load()

        # Prepare data
        X, y = self.prepare_data(df, is_training=True)

        # Check class distribution
        unique_classes, counts = np.unique(y, return_counts=True)
        class_dist = dict(zip(unique_classes, counts))
        print(f"üìä Class distribution: {class_dist}")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        # Store test sample for SHAP
        self.X_test_sample = X_test.copy()

        print(f"üìà Training set size: {len(y_train)}")
        print(f"üìà Test set size: {len(y_test)}")

        # Initialize model
        print("üöÄ Training new model...")
        self.model = XGBClassifier(
            n_estimators=1000,        # same as LGBM
            learning_rate=0.05,       # same as LGBM
            random_state=random_state,
            verbosity=0,              # equivalent to verbosity=-1 in LGBM
            objective="binary:logistic",  # binary classification objective
            booster="gbtree",         # gbdt equivalent
            max_depth=6,              # same depth limit
            subsample=0.8,            # bagging_fraction
            colsample_bytree=0.8,     # feature_fraction
            reg_lambda=1.0,           # default L2 reg (LGBM implicit)
            reg_alpha=0.0,            # default L1 reg
            min_child_weight=1,       # similar to LGBM min_data_in_leaf (not exact)
            gamma=0,                  # split regularization (roughly like LGBM min_gain_to_split)
            n_jobs=-1,                # parallel processing
            tree_method="hist"        # speed comparable to LightGBM's histogram algorithm
        )

        # Train model with early stopping
        try:
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=False
            )
            print("‚úÖ Training complete")
        except Exception as e:
            print(f"‚ùå Training error: {e}")
            # Fallback to basic training
            self.model.fit(X_train, y_train)
            print("‚úÖ Training complete (fallback mode)")

        # Make predictions
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]

        # Create comprehensive visualizations
        self.plot_model_performance(y_test, y_pred, y_pred_proba)

        # Initialize SHAP explainer
        self.initialize_explainer(X_train)

        # Create SHAP summary plots
        if self.explainer is not None:
            self.plot_shap_summary(X_test)

        # Save artifacts
        self.save()

    def predict_single(self, patient_dict, explain=True):
        """Predict for a single patient with optional explanation"""
        if self.model is None:
            raise ValueError("Model not loaded. Please train or load a model first.")

        if explain and self.explainer is not None:
            return self.explain_patient_prediction(patient_dict)
        else:
            try:
                # Convert to DataFrame
                df = pd.DataFrame([patient_dict])

                # Prepare data
                X, _ = self.prepare_data(df, is_training=False)

                # Make prediction
                pred_proba = self.model.predict_proba(X)
                pred = self.model.predict(X)

                # Return results
                probability = pred_proba[0][1] if pred_proba.shape[1] > 1 else pred_proba[0][0]
                prediction = int(pred[0])

                return prediction, probability, None

            except Exception as e:
                print(f"‚ùå Prediction error: {e}")
                return 0, 0.0, None

    def predict_batch(self, df):
        """Predict for multiple patients"""
        if self.model is None:
            raise ValueError("Model not loaded. Please train or load a model first.")

        try:
            X, _ = self.prepare_data(df, is_training=False)
            pred_proba = self.model.predict_proba(X)
            preds = self.model.predict(X)

            probabilities = pred_proba[:, 1] if pred_proba.shape[1] > 1 else pred_proba[:, 0]

            return preds, probabilities
        except Exception as e:
            print(f"‚ùå Batch prediction error: {e}")
            return np.array([]), np.array([])

    def get_model_info(self):
        """Get information about the trained model"""
        if self.model is None:
            return "No model loaded"

        info = {
            "model_type": "LightGBM Classifier",
            "n_features": len(self.feature_names) if self.feature_names else "Unknown",
            "numeric_features": len(self.numeric_cols),
            "categorical_features": len(self.categorical_cols),
            "target_column": self.target_col,
            "shap_available": self.explainer is not None
        }
        return info

    def feature_analysis(self, top_n=20):
        """Analyze feature importance and correlations"""
        if self.model is None or self.feature_names is None:
            print("‚ùå Model not trained or feature names not available")
            return

        try:
            # Feature importance
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)

            print(f"\nüéØ TOP {top_n} MOST IMPORTANT FEATURES:")
            print("-" * 50)
            for idx, row in importance_df.head(top_n).iterrows():
                feature_type = "Numeric" if row['feature'] in self.numeric_cols else "Categorical"
                print(f"  {row['feature']:.<40} {row['importance']:.4f} ({feature_type})")

            # Visualization
            plt.figure(figsize=(10, max(8, top_n * 0.4)))
            top_features = importance_df.head(top_n)
            plt.barh(range(len(top_features)), top_features['importance'])
            plt.yticks(range(len(top_features)), top_features['feature'])
            plt.xlabel('Feature Importance')
            plt.title(f'Top {top_n} Feature Importances')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.show()

            return importance_df

        except Exception as e:
            print(f"‚ùå Error in feature analysis: {e}")
            return None


def main():
    """Main execution function with enhanced error handling"""
    # File paths - update these to match your environment
    data_path = "/content/sample_data/gallstone_.csv"
    model_path = "/content/gallstone_xgb.pkl"
    prep_path = "/content/gallstone_xgb_prep.pkl"

    try:
        # Initialize trainer
        trainer = LightGBMContinuousTrainer(data_path, model_path, prep_path)

        # Load and train
        df = trainer.load_data()
        trainer.train(df)

        # Show model information
        print("\n" + "="*60)
        print("üìä MODEL INFORMATION")
        print("="*60)
        info = trainer.get_model_info()
        for key, value in info.items():
            print(f"{key.replace('_', ' ').title()}: {value}")

        # Feature analysis
        trainer.feature_analysis(top_n=15)

        # Example prediction with comprehensive explanation
        patient_example = {
            'Age': 45, 'Gender': 'Male', 'Comorbidity': None,
            'Coronary Artery Disease (CAD)': 'No', 'Hypothyroidism': 'No',
            'Hyperlipidemia': 'No', 'Diabetes Mellitus (DM)': 'No',
            'Height': 170, 'Weight': 70, 'Body Mass Index (BMI)': 24.2,
            'Total Body Water (TBW)': 40, 'Extracellular Water (ECW)': 15,
            'Intracellular Water (ICW)': 25,
            'Extracellular Fluid/Total Body Water (ECF/TBW)': 0.375,
            'Total Body Fat Ratio (TBFR) (%)': 20, 'Lean Mass (LM) (%)': 70,
            'Body Protein Content (Protein) (%)': 15, 'Visceral Fat Rating (VFR)': 10,
            'Bone Mass (BM)': 3, 'Muscle Mass (MM)': 35, 'Obesity (%)': 15,
            'Total Fat Content (TFC)': 14, 'Visceral Fat Area (VFA)': 80,
            'Visceral Muscle Area (VMA) (Kg)': 30, 'Hepatic Fat Accumulation (HFA)': 5,
            'Glucose': 90, 'Total Cholesterol (TC)': 180,
            'Low Density Lipoprotein (LDL)': 100, 'High Density Lipoprotein (HDL)': 50,
            'Triglyceride': 120, 'Aspartat Aminotransferaz (AST)': 25,
            'Alanin Aminotransferaz (ALT)': 22, 'Alkaline Phosphatase (ALP)': 80,
            'Creatinine': 1.0, 'Glomerular Filtration Rate (GFR)': 90,
            'C-Reactive Protein (CRP)': 0.3, 'Hemoglobin (HGB)': 14, 'Vitamin D': 30
        }

        print("\nüîÆ Making prediction with detailed explanation for example patient...")
        pred, proba, shap_vals = trainer.predict_single(patient_example, explain=True)

        # Additional analysis for high-risk patients
        if proba and proba > 0.5:
            print("\n‚ö†Ô∏è  HIGH RISK PATIENT DETECTED!")
            print("Consider additional screening or preventive measures.")
        elif proba and proba > 0.3:
            print("\nüìä MODERATE RISK PATIENT")
            print("Regular monitoring recommended.")
        else:
            print("\n‚úÖ LOW RISK PATIENT")
            print("Standard preventive care recommended.")

        print(f"\nüéØ Model ready for production use!")
        print(f"üìÅ Model saved to: {model_path}")
        print(f"üìÅ Preprocessing saved to: {prep_path}")

    except FileNotFoundError as e:
        print(f"‚ùå File not found: {e}")
        print("Please ensure the data file exists at the specified path.")
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

import xgboost as xgb
print(xgb.__version__)

pip install pandas matplotlib numpy plotly scikit-learn xgboost lightgbm matplotlib seaborn shap joblib streamlit